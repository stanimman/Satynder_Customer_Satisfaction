https://colab.research.google.com/drive/1USjnH9kBSVC9TanyHTDAeJB0nBLti-Ek

http://forums.fast.ai/t/colaboratory-and-fastai/10122/17



# coding: utf-8

# In[1]:


import numpy as np
import pandas as pd


# In[2]:


# Loading your data
def load_data(path,format):
    if (format == 'csv'):
        df = pd.read_csv(path)
    else: 
        df = "Path / Data Error"
    return df


# In[3]:


path = "C:/Users/584140/Documents/Santander/train.csv"


# In[4]:


df_train = load_data(path,'csv')


# In[5]:


# Basic Data Exploration
def basic_exploration(df):
    #print ("Shape of the Dataframe is  %f"(df.shape)) Not working need to explore
    print(df.shape)
    #print (df.dtypes)
    print (df.info())


# In[6]:


basic_exploration(df_train)


# In[7]:


# Count NA / 
def na_count(df):
    countNA_Column = (df.isnull().sum()).sum()
    String1 = "No. of NA columns = "
    hw12 = '%s %d' % (String1, countNA_Column)
    if (countNA_Column > 0):
        print(df.isnull().sum()>0)
    print(hw12)
    
    
    
    
    


# In[8]:


type(df_train.isnull().sum())


# In[9]:


na_count(df_train)


# In[10]:


print ((df_train.isnull().sum()).sum())


# In[11]:


# View Data Type of each column and the names
df_train.dtypes
df_train.columns


# In[12]:


#List unique values in the df['name'] column
df_train.TARGET.unique()


# In[13]:


#iloc refers to the index of the column , use loc['TARGET'] to subset based on the some condition
# https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/
#X_train = df_train.drop('TARGET', axis=1)
X_train = df_train.iloc[0:25000,0:(df_train.shape[1]-1)]
y_train = df_train.iloc[0:25000,-1]
print(X_train.shape)
print(y_train.shape)


# In[14]:


from sklearn.metrics import confusion_matrix
from sklearn.linear_model.logistic import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import roc_curve, auc

classifier = LogisticRegression()
classifier.fit(X_train, y_train)
scores = cross_val_score(classifier, X_train, y_train, cv=5)
print('Accuracies: %s' % scores)
print('Mean accuracy: %s' % np.mean(scores))


# In[15]:


y_pred = classifier.predict(X_train)


# In[18]:


# Plot Confusion Matrix

import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_train, y_pred)
print(confusion_matrix)
plt.matshow(confusion_matrix)
plt.title('Confusion matrix')
plt.colorbar()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()


# In[19]:


unique_elements, counts_elements = np.unique(y_pred, return_counts=True)
print(np.asarray((unique_elements, counts_elements)))


# In[20]:


unique_elements, counts_elements = np.unique(y_train, return_counts=True)
print(np.asarray((unique_elements, counts_elements)))


# In[23]:


precisions = cross_val_score(classifier, X_train, y_train, cv=5,
  scoring='precision')
#print('Precision: %s' % np.mean(precisions))
print(precisions)
recalls = cross_val_score(classifier, X_train, y_train, cv=5,
  scoring='recall')
print('Recall: %s' % np.mean(recalls))
print(recalls)



# Hello World program in Python
    
import numpy as np
N, D1, D2, D3 = 20, 5, 6, 3
eps =0.02
X = np.random.randn(N, D1)
D = X.shape[1]
gamma=1
beta=0

#mu = 1./N * np.sum(X, axis = 0)
#xmu = X - mu
#sq = xmu ** 2
#var = 1./N * np.sum(sq, axis = 0)
#sqrtvar = np.sqrt(var + eps)
#ivar = 1./sqrtvar
#xhat = xmu * ivar
#gammax = gamma * xhat
#out = gammax + beta

#print(out)


sample_mean_mat = np.mean(X,axis=0)
mean_mat_stretch = np.tile(sample_mean_mat,(X.shape[0],1)
#sq = xmu ** 2
#sample_variance_mat = 1./N * np.sum(sq, axis = 0)
#sample_variance_mat = np.var(X,axis=0,dtype=np.float64)
#sample_sd_mat = np.sqrt(sample_variance_mat + eps)
#variance_mat_stretch = np.tile(variance_mat_stretch,(X.shape[0],1)
#Batch_norm = ( X - mean_mat_stretch ) / variance_mat_stretch
#gamma_stretch = np.tile(gamma,(X.shape[0],1)
#beta_stretch = np.tile(beta,(X.shape[0],1)                        
#out = gamma*batch_norm + beta_stretch  
#print(out)




sample_mean_mat = np.mean(X,axis=0)
#mean_mat_stretch = np.tile(sample_mean_mat,(X.shape[0],1)
#sq = xmu ** 2
#sample_variance_mat = 1./N * np.sum(sq, axis = 0)
sample_variance_mat = np.var(X,axis=0,dtype=np.float64)
sample_sd_mat = np.sqrt(sample_variance_mat + eps)
inv_varx = 1./sample_sd_mat # Seperating out as they are required in backprop
#variance_mat_stretch = np.tile(variance_mat_stretch,(X.shape[0],1)
Num = ( X - sample_mean_mat ) #Seperating out as they are required backprop
batch_norm =  Num / sample_sd_mat
#gamma_stretch = np.tile(gamma,(X.shape[0],1)
#beta_stretch = np.tile(beta,(X.shape[0],1)                        
out1 = gamma*batch_norm + beta  
print(out1)


sample_sd_mat = np.sqrt(sample_variance_mat + eps)
dx1 = dout * gamma * inv_varx
#dx2 = -1. * 1./N * dx1 
dvar = -1. * dout * Num * inv_varx * inv_varx 
dx2 = 1./N * 2 * Num * 0.5 * (1. / np.sqrt(sample_sd_mat)) * dvar
#dx4 = -1. * 1./N * dx3
dx = dx1 + dx2
dx =  -1. * 1./N * dx
print(dx) 


#get the dimensions of the input/output
N,D = dout.shape

#step9
dbeta = np.sum(dout, axis=0)
dgammax = dout #not necessary, but more understandable

#step8
dgamma = np.sum(dgammax*xhat, axis=0)
dxhat = dgammax * gamma

#step7
divar = np.sum(dxhat*xmu, axis=0)
dxmu1 = dxhat * ivar

#step6
dsqrtvar = -1. /(sqrtvar**2) * divar

#step5
dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar

#step4
dsq = 1. /N * np.ones((N,D)) * dvar

#step3
dxmu2 = 2 * xmu * dsq

#step2
dx1 = (dxmu1 + dxmu2)
dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)

#step1
dx2 = 1. /N * np.ones((N,D)) * dmu

#step0
dx = dx1 + dx2

print dx

python_Learning

numpy.random.rand : Taking random sample from uniform distribution / rectangular distribution 

http://mathworld.wolfram.com/UniformDistribution.html

Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).

numpy.random.randn : Taking random sample from normal distribution

Slicing / subsetting-a-2d-numpy-array

Try executing the manual code , gives good idea on slicing on 2d array

explains the np.meshgrid and np.ix_ effort lessly

https://stackoverflow.com/questions/30917753/subsetting-a-2d-numpy-array

What is differentiating
1. A blog which gives explicit evidence (even if it repetition of what is available in Internet)
2. A neatly build resume (First impression is going make all the different)
	Exhibit the hands on experience - all are course oriented
	Exhibit the Big Data Expertise - not many in the industry knows the stuff
3. Exhibiting self confidence in Interview as though you have dealt with it well before (Not too excited but a matured behavior)
   
 x1 = np.pad(x,((0,0),(0,0),(pad,pad),(pad,pad)),'constant', constant_values=(0))

import numpy as np
x_shape = (2, 3, 4, 4)
w_shape = (3, 3, 4, 4)
x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)
w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)
b = np.linspace(-0.1, 0.2, num=3)

#print(x)
#print(x.shape)
pad = 1
x1 = np.pad(x,((0,0),(0,0),(pad,pad),(pad,pad)),'constant', constant_values=(0))
#x1 = np.pad(x,((0,0),(0,0),(2,2),(2,2)),'constant', constant_values=(0))
#print(x1.shape)
#  print(x1)
s = 2
st = 0
for i in range(N)

slicedx1 = x1[i,:,st:F+1,st:F+1]

st += s 
#print(slicedx1)
#print(slicedx1.shape)

x2 = np.reshape(slicedx1,(-1))
print(x2)
